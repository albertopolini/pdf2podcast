{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF2Podcast Test Notebook\n",
    "\n",
    "Questo notebook dimostra le funzionalità della libreria pdf2podcast, incluse:\n",
    "- Estrazione testo da PDF\n",
    "- Chunking del testo\n",
    "- Ricerca semantica\n",
    "- Generazione podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\polin\\OneDrive - Devnut\\Documenti\\Progetti\\9.Demo\\pdf2podcast\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Aggiungi il percorso corretto per i moduli\n",
    "module_path = str(Path(os.getcwd()).parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import diretti dai moduli locali\n",
    "from pdf2podcast import PodcastGenerator\n",
    "from pdf2podcast.core.rag import AdvancedPDFProcessor as SimplePDFProcessor\n",
    "from pdf2podcast.core.llm import GeminiLLM\n",
    "from pdf2podcast.core.tts import AWSPollyTTS, GoogleTTS\n",
    "from pdf2podcast.core.prompts import PodcastPromptBuilder\n",
    "from pdf2podcast.core.processing import SimpleChunker, SemanticRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Setup iniziale e configurazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica le variabili d'ambiente\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Verifica le chiavi necessarie\n",
    "api_key = os.getenv(\"GENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GENAI_API_KEY non trovata nel file .env\")\n",
    "\n",
    "# Verifica che esista il PDF di test\n",
    "PDF_PATH = \"./test2.pdf\"\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    raise ValueError(f\"File PDF non trovato: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Base: Estrazione Testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lunghezza testo estratto: 39685\n",
      "\n",
      "Primi 500 caratteri:\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz K\n"
     ]
    }
   ],
   "source": [
    "# Inizializza il processore PDF base\n",
    "processor = SimplePDFProcessor(\n",
    "    max_chars_per_chunk=4000,\n",
    "    extract_images=True,\n",
    "    metadata=True\n",
    ")\n",
    "\n",
    "# Estrai il testo\n",
    "text = processor.process_document(PDF_PATH)\n",
    "print(\"Lunghezza testo estratto:\", len(text))\n",
    "print(\"\\nPrimi 500 caratteri:\\n\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di chunks: 411\n",
      "\n",
      "Primo chunk:\n",
      "\n",
      "Understanding Deep Learning Simon J.D. Prince November 21, 2024 The most recent version of this document can be found at http://udlbook.com. Copyright in this work has been licensed exclusively to The MIT Press, https://mitpress.mit.edu, which released the final version to the public in December 2023. Inquiries regarding rights should be addressed to the MIT Press, Rights & Permissions Department. This work is subject to a Creative Commons CC-BY-NC-ND license. I would really appreciate help improving this document. No detail too small! Please contact me with suggestions, factual inaccuracies, ambiguities, questions, and errata via github or by e-mail at udlbookmail@gmail.com.\n",
      "This book is dedicated to Blair, Calvert, Coppola, Ellison, Faulkner, Kerpatenko, Morris, Robinson, Sträussler, Wallace, Waymon, Wojnarowicz, and all the others whose work is even more important and interesting than deep learning.\n",
      "Contents Preface ix Acknowledgements xi 1 Introduction 1 1.1 Supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.3 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.4 Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.5 Structure of book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.6 Other books . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.7 How to read this book . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2 Supervised learning 17 2.1 Supervised learning overview . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.2 Linear regression example . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3 Shallow neural networks 25 3.1 Neural network example . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2 Universal approximation theorem . . . . . . . . . . . . . . . . . . . . . . 29 3.3 Multivariate inputs and outputs . . . . . . . . . . . . . . . . . . . . . . . 30 3.4 Shallow neural networks: general case . . . . . . . . . . . . . . . . . . . . 33 3.5 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4 Deep neural networks 41 4.1 Composing neural networks . . . . . . . . . . . . . . . . . . . . . . . . . 41 4.2 From composing networks to deep networks . . . . . . . . . . . . . . . . 43 4.3 Deep neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.4 Matrix notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.5 Shallow vs. deep neural networks . . . . . . . . . . . . . . . . . . . . . . 49 4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 Draft: please send errata to udlbookmail@gmail.com.\n"
     ]
    }
   ],
   "source": [
    "# Inizializza il chunker\n",
    "chunker = SimpleChunker()\n",
    "\n",
    "# Dividi il testo in chunks\n",
    "chunks = chunker.chunk_text(text, chunk_size=4000)\n",
    "\n",
    "print(f\"Numero di chunks: {len(chunks)}\")\n",
    "print(\"\\nPrimo chunk:\\n\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Ricerca Semantica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks più rilevanti per la query: Explain the main concepts of the paper\n",
      "\n",
      "Chunk 1:\n",
      "16 1 Introduction is Reinforcement Learning: An Introduction (Sutton & Barto, 2018). A good initial resource is Foundations of Deep Reinforcement Learning (Graesser & Keng, 2019). 1.7 How to read this book Most remaining chapters in this book contain a main body of text, a notes section, and a set of problems. The main body of the text is intended to be self-contained and can be read without recourse to the other parts of the chapter. As much as possible, background mathematics is incorporated into the main body of the text. However, for larger topics that would be a distraction to the main thread of the argument, the background material is appendicized, and a reference is provided in the margin. Most notation in this book is Appendix A Notation standard. However, some conventions are less widely used, and the reader is encouraged to consult appendix A before proceeding. The main body of text includes many novel illustrations and visualizations of deep learning models and results. I’ve worked hard to provide new explanations of existing ideas rather than merely curate the work of others. Deep learning is a new field, and sometimes phenomena are poorly understood. I try to make it clear where this is the case and when my explanations should be treated with caution. References are included in the main body of the chapter only where results are de- picted. Instead, they can be found in the notes section at the end of the chapter. I do not generally respect historical precedent in the main text; if an ancestor of a current technique is no longer useful, then I will not mention it. However, the historical develop- ment of the field is described in the notes section, and hopefully, credit is fairly assigned. The notes are organized into paragraphs and provide pointers for further reading. They should help the reader orient themselves within the sub-area and understand how it re- lates to other parts of machine learning. The notes are less self-contained than the main text. Depending on your level of background knowledge and interest, you may find these sections more or less useful. Each chapter has a number of associated problems. They are referenced in the margin of the main text at the point that they should be attempted. As George Pólya noted, “Mathematics, you see, is not a spectator sport.” He was correct, and I highly recommend that you attempt the problems as you go. In some cases, they provide insights that will help you understand the main text. Problems for which the answers are provided on the associated website (http://udlbook.com) are indicated with an asterisk. Additionally, Python notebooks that will help you understand the ideas in this book are also available via the website, and these are also referenced in the margins of the text. Indeed, if Notebook 1.1 Background mathematics you are feeling rusty, it might be worth working through the notebook on background mathematics right now. Unfortunately, the pace of research in AI makes it inevitable that this book will be a constant work in progress. If there are parts you find hard to understand, notable omis- sions, or sections that seem extraneous, please get in touch via the associated website. Together, we can make the next edition better. This work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n",
      "\n",
      "Chunk 2:\n",
      "13.1 What is a graph? 241 Figure 13.1 Real-world graphs. Some objects, such as a) road networks, b) molecules, and c) electrical circuits, are naturally structured as graphs.  Social networks are graphs where nodes are people, and the edges represent friend- ships between them.  The scientific literature can be viewed as a graph where the nodes are papers, and the edges represent citations.  Wikipedia can be considered a graph where the nodes are articles, and the edges represent hyperlinks between articles.  Computer programs can be represented as graphs where the nodes are syntax tokens (variables at different points in the program flow), and the edges represent computations involving these variables.  Geometric point clouds can be represented as graphs. Here, each point is a node with edges connecting to other nearby points.  Protein interactions in a cell can be expressed as graphs, where the nodes are the proteins, and there is an edge between two proteins if they interact. In addition, a set (an unordered list) can be treated as a graph in which every member is a node and connects to every other. An image can be treated as a graph with regular topology, in which each pixel is a node with edges to the adjacent pixels. 13.1.1 Types of graphs Graphs can be categorized in various ways. The social network in figure 13.2a contains undirected edges; each pair of individuals with a connection between them have mutually agreed to be friends, so there is no sense that the relationship is directional. In contrast, the citation network in figure 13.2b contains directed edges. Each paper cites other papers, and this relationship is inherently one-way. Figure 13.2c depicts a knowledge graph that encodes a set of facts about objects by defining relations between them. Technically, this is a directed heterogeneous multigraph. It is heterogeneous because the nodes can represent different types of entities (e.g., people, countries, companies). It is a multigraph because there can be multiple edges of different types between any two nodes. Draft: please send errata to udlbookmail@gmail.com.\n",
      "242 13 Graph neural networks Figure 13.2 Types of graphs. a) A social network is an undirected graph; the connections between people are symmetric. b) A citation network is a directed graph; one publication cites another, so the relationship is asymmetric. c) A knowledge graph is a directed heterogeneous multigraph. The nodes are hetero- geneous in that they represent different object types (people, places, companies) and multiple edges may represent different relations between each node. d) A point set can be converted to a graph by forming edges between nearby points. Each node has an associated position in 3D space, and this is termed a geometric graph (adapted from Hu et al., 2022). e) The scene on the left can be represented by a hierarchical graph. The topology of the room, table, and light are all repre- sented by graphs. These graphs form nodes in a larger graph representing object adjacency (adapted from Fernández-Madrigal & González, 2002). This work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n",
      "\n",
      "Chunk 3:\n",
      "Chapter 2 Supervised learning A supervised learning model defines a mapping from one or more inputs to one or more outputs. For example, the input might be the age and mileage of a second-hand Toyota Prius, and the output might be the estimated value of the car in dollars. The model is just a mathematical equation; when the inputs are passed through this equation, it computes the output, and this is termed inference. The model equation also contains parameters. Different parameter values change the outcome of the computa- tion; the model equation describes a family of possible relationships between inputs and outputs, and the parameters specify the particular relationship. When we train or learn a model, we find parameters that describe the true relationship between inputs and outputs. A learning algorithm takes a training set of input/output pairs and manipulates the parameters until the inputs predict their corresponding out- puts as closely as possible. If the model works well for these training pairs, then we hope it will make good predictions for new inputs where the true output is unknown. The goal of this chapter is to expand on these ideas. First, we describe this framework more formally and introduce some notation. Then we work through a simple example in which we use a straight line to describe the relationship between input and output. This linear model is both familiar and easy to visualize, but nevertheless illustrates all the main ideas of supervised learning. 2.1 Supervised learning overview In supervised learning, we aim to build a model that takes an input x and outputs a prediction y. For simplicity, we assume that both the input x and output y are vectors of a predetermined and fixed size and that the elements of each vector are always ordered in the same way; in the Prius example above, the input x would always contain the age of the car and then the mileage, in that order. This is termed structured or tabular data. To make the prediction, we need a model f[] that takes input x and returns y, so: y = f[x]. (2.1) Draft: please send errata to udlbookmail@gmail.com.\n",
      "\n",
      "Chunk 4:\n",
      "Chapter 1 Introduction Artificial intelligence, or AI, is concerned with building systems that simulate intelligent behavior. It encompasses a wide range of approaches, including those based on logic, search, and probabilistic reasoning. Machine learning is a subset of AI that learns to make decisions by fitting mathematical models to observed data. This area has seen explosive growth and is now (incorrectly) almost synonymous with the term AI. A deep neural network (or deep network for short) is a type of machine learning model, and the process of fitting these models to data is referred to as deep learning. At the time of writing, deep networks are the most powerful and practical machine learning models and are often encountered in day-to-day life. It is commonplace to translate text to another language using a natural language processing algorithm, to search for images of a given object using a computer vision system, or to converse with a digital assistant via a speech recognition interface. All of these applications are powered by deep learning. As the title suggests, this book aims to help a reader new to this field understand the principles behind deep learning. The book is neither terribly theoretical (there are no proofs) nor extremely practical (there is almost no code). The goal is to explain the underlying ideas; after consuming this volume, the reader will be able to apply deep learning to novel situations where there is no existing recipe for success. Machine learning methods can coarsely be divided into three areas: supervised, unsu- pervised, and reinforcement learning. At the time of writing, the cutting-edge methods in all three areas rely on deep learning (figure 1.1). This introductory chapter describes these three areas at a high level, and this taxonomy is also loosely reflected in the book’s organization. Whether we like it or not, deep learning is poised to change our world, and this change will not all be positive. Hence, this chapter also contains a brief primer on AI ethics. We conclude with advice on how to make the most of this book. 1.1 Supervised learning Supervised learning models define a mapping from input data to an output prediction. In the following sections, we discuss the inputs, the outputs, the model itself, and what is meant by “training” a model. Draft: please send errata to udlbookmail@gmail.com.\n",
      "\n",
      "Chunk 5:\n",
      "18.2 Encoder (forward process) 349 Figure 18.1 Diffusion models. The encoder (forward, or diffusion process) maps the input x through a series of latent variables z1 . . . zT . This process is pre- specified and gradually mixes the data with noise until only noise remains. The decoder (reverse process) is learned and passes the data back through the la- tent variables, removing noise at each stage. After training, new examples are generated by sampling noise vectors zT and passing them through the decoder. adjacent pair of latent variables zt and zt−1. The loss function encourages each network to invert the corresponding encoder step. The result is that noise is gradually removed from the representation until a realistic-looking data example remains. To generate a new data example x, we draw a sample from q(zT ) and pass it through the decoder. In section 18.2, we consider the encoder in detail. Its properties are non-obvious but are critical for the learning algorithm. In section 18.3, we discuss the decoder. Section 18.4 derives the training algorithm, and section 18.5 reformulates it to be more practical. Section 18.6 discusses implementation details, including how to make the generation conditional on text prompts. 18.2 Encoder (forward process) The diffusion or forward process1 (figure 18.2) maps a data example x through a series of intermediate variables z1, z2, . . . , zT with the same size as x according to: z1 = p 1 −β1 · x + p β1 · ϵ1 (18.1) zt = p 1 −βt · zt−1 + p βt · ϵt ∀t ∈2, . . . , T, where ϵt is noise drawn from a standard normal distribution. The first term attenuates the data plus any noise added so far, and the second adds more noise. The hyperparam- eters βt ∈[0, 1] determine how quickly the noise is blended and are collectively known as the noise schedule. The forward process can equivalently be written as: 1Note, this is the opposite nomenclature to normalizing flows, where the inverse mapping moves from the data to the latent variable, and the forward mapping moves back again. Draft: please send errata to udlbookmail@gmail.com.\n",
      "350 18 Diffusion models Figure 18.2 Forward process. a) We consider one-dimensional data x with T = 100 latent variables z1, . . . , z100 and β = 0.03 at all steps. Three values of x (gray, cyan, and orange) are initialized (top row). These are propagated through z1, . . . , z100. At each step, the variable is updated by attenuating its value by √1 −β and adding noise with mean zero and variance β (equation 18.1). Accordingly, the three examples noisily propagate through the variables with a tendency to move toward zero. b) The conditional probabilities Pr(z1|x) and Pr(zt|zt−1) are normal distributions with a mean that is slightly closer to zero than the current point and a fixed variance βt (equation 18.2). q(z1|x) = Normz1 hp 1 −β1x, β1I i (18.2) q(zt|zt−1) = Normzt hp 1 −βtzt−1, βtI i ∀t ∈{2, . . . , T}. This is a Markov chain because the probability of zt is determined entirely by the value of the immediately preceding variable zt−1. With suﬀicient steps T, all traces of the original data are removed, and q(zT |x) = q(zT ) becomes a standard normal distribution.2 Problem 18.1 The joint distribution of all of the latent variables z1, z2, . . . , zT given input x is: q(z1...T |x) = q(z1|x) T Y t=2 q(zt|zt−1). (18.3) 2We use q(zt|zt−1) rather than Pr(zt|zt−1) to match the notation in the description of the VAE encoder in the previous chapter. This work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n"
     ]
    }
   ],
   "source": [
    "# Inizializza il retriever\n",
    "retriever = SemanticRetriever()\n",
    "\n",
    "# Aggiungi i chunks al retriever\n",
    "retriever.add_texts(chunks)\n",
    "\n",
    "# Prova una query\n",
    "query = \"Explain the main concepts of the paper\"\n",
    "relevant_chunks = retriever.get_relevant_chunks(query, k=5)\n",
    "\n",
    "print(\"Chunks più rilevanti per la query:\", query)\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Completo: Generazione Podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script generato:\n",
      "\n",
      "The ideal of a perfect legal system, a system devoid of bias, error, and inefficiency, remains a perpetually elusive aspiration.  The inherent complexities of human interaction, the subjective interpretation of laws, and the limitations of even the most meticulously crafted legal frameworks conspire to create a persistent gap between this ideal and the reality of legal practice. This discrepancy, far from being a mere philosophical abstraction, represents a significant challenge with profound societal implications.  Consider, for instance, the disparities in sentencing for similar crimes, often influenced by factors extraneous to the legal merits of the case – socioeconomic background, racial bias, or even the perceived demeanor of the defendant.  These inconsistencies highlight the inherent limitations of a system striving for objectivity within a fundamentally subjective human context.  The pursuit of justice, therefore, necessitates a constant critical examination of legal processes, a relentless striving for improvement, and a commitment to addressing systemic biases that undermine the fairness and equity of the system.  This requires not only legal reform but also a deeper understanding of the cognitive and social factors that influence legal decision-making.\n",
      "\n",
      "The complexities of human language mirror, in many ways, the complexities of the legal system.  Natural language processing (NLP), a field dedicated to enabling computers to understand, interpret, and generate human language, offers a fascinating parallel.  The intricacies of human communication, with its nuances, ambiguities, and contextual dependencies, present a formidable challenge for computational models.  Yet, recent advancements in NLP, particularly the development of sophisticated neural network architectures, have yielded remarkable progress in tackling these challenges.  Attention mechanisms, a key innovation in these models, provide a particularly illuminating example of the complex computational processes involved in understanding language.\n",
      "\n",
      "These mechanisms, integral to the functioning of advanced language models such as transformers, operate by assigning weights to different parts of an input sequence, effectively focusing on the most relevant information for a given task.  Instead of processing the entire input uniformly, attention mechanisms allow the model to dynamically prioritize certain words or phrases, thereby capturing intricate relationships between different elements of the text.  This dynamic weighting is not a monolithic process; rather, it is achieved through a multitude of individual attention heads, each specializing in different aspects of language processing.\n",
      "\n",
      "Consider, for example, the task of machine translation.  In translating a sentence from English to French, one attention head might focus on identifying the grammatical subject of the sentence, another might concentrate on verb conjugation, while yet another might specialize in resolving pronoun references.  These heads operate concurrently, their individual contributions integrated to produce a coherent and accurate translation.  The specialization of attention heads is not pre-programmed; rather, it emerges through the training process, a testament to the model's ability to learn complex patterns and relationships within the data.  This emergent specialization is not limited to machine translation; it extends to other NLP tasks such as text summarization, question answering, and sentiment analysis.\n",
      "\n",
      "In text summarization, for instance, some attention heads might focus on identifying the most important sentences in a document, while others might concentrate on extracting key phrases or concepts.  In question answering, attention heads might specialize in identifying the relevant parts of a text passage that contain the answer to a given question.  In sentiment analysis, attention heads might focus on identifying words or phrases that express positive or negative emotions.  The remarkable capacity of these models to adapt to diverse tasks underscores the power and flexibility of the attention mechanism.\n",
      "\n",
      "The behavior of these attention heads is not merely a technical curiosity; it offers valuable insights into the cognitive processes involved in human language understanding.  The specialization of attention heads mirrors, in some ways, the modularity of the human brain, where different regions are specialized for different cognitive functions.  The intricate interplay between these specialized modules allows for the seamless integration of information from various sources, enabling a comprehensive understanding of complex linguistic structures.  The study of attention mechanisms, therefore, provides a unique window into the workings of both artificial and natural intelligence.\n",
      "\n",
      "Furthermore, the development and refinement of these attention mechanisms are not merely academic exercises; they have profound practical implications.  The accuracy and efficiency of various NLP applications, from machine translation systems used by millions daily to sophisticated chatbots assisting customer service, are directly dependent on the effectiveness of these mechanisms.  Improvements in attention mechanisms translate directly into improvements in the performance of these applications, leading to more accurate translations, more informative summaries, and more effective interactions with AI systems.  The pursuit of better attention mechanisms, therefore, is not just a pursuit of better algorithms; it is a pursuit of better tools for communication, understanding, and information processing.\n",
      "\n",
      "The parallel between the pursuit of a perfect legal system and the development of perfect natural language processing models is striking.  Both endeavors grapple with inherent complexities, striving for an ideal that remains perpetually out of reach.  Yet, the pursuit itself is crucial.  The relentless striving for improvement, the constant critical examination of processes, and the commitment to addressing limitations are essential for progress in both fields.  Just as the legal system benefits from ongoing reform and a deeper understanding of human behavior, so too does NLP benefit from the development of more sophisticated models and a deeper understanding of the cognitive processes involved in language understanding.  The journey towards perfection, in both domains, is a journey of continuous learning, adaptation, and refinement.  The imperfections, however, should not discourage the pursuit; rather, they should serve as a constant reminder of the challenges and the opportunities that lie ahead.  The pursuit of justice, like the pursuit of perfect language understanding, is a continuous process, a testament to the enduring human desire for clarity, accuracy, and fairness.\n",
      "\n",
      "Dettagli audio:\n",
      "File: output.mp3\n",
      "Dimensione: 2084780 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6824"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test Completo: Generazione Podcast\n",
    "chunker = SimpleChunker()\n",
    "retriever = SemanticRetriever()\n",
    "prompt_builder = PodcastPromptBuilder()\n",
    "\n",
    "# Configura il processore PDF\n",
    "processor = SimplePDFProcessor(\n",
    "    chunker=chunker,\n",
    "    retriever=retriever,\n",
    "    extract_images=True,\n",
    "    max_chars_per_chunk=6000,\n",
    "    metadata=True\n",
    ")\n",
    "\n",
    "# Crea il generator con la nuova configurazione basata sui manager\n",
    "generator = PodcastGenerator(\n",
    "    rag_system=processor,\n",
    "    llm_type=\"gemini\",\n",
    "    tts_type=\"google\",  # Using Google TTS for testing\n",
    "    llm_config={\n",
    "        \"api_key\": api_key,\n",
    "        \"max_output_tokens\": 8000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"prompt_builder\": prompt_builder\n",
    "    },\n",
    "    tts_config={\n",
    "        \"language\": \"en\",\n",
    "        \"tld\": \"com\",\n",
    "        \"slow\": False\n",
    "    },\n",
    "    chunker=chunker,\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# Genera il podcast con una query specifica\n",
    "result = generator.generate(\n",
    "    pdf_path=PDF_PATH,\n",
    "    output_path=\"output.mp3\",\n",
    "    complexity=\"advanced\",\n",
    "    audience=\"experts\",\n",
    "    query=\"Explain the main concepts of the paper and the main results\"\n",
    ")\n",
    "\n",
    "# Mostra i risultati\n",
    "print(\"Script generato:\\n\")\n",
    "print(result[\"script\"])\n",
    "\n",
    "print(\"\\nDettagli audio:\")\n",
    "print(f\"File: {result['audio']['path']}\")\n",
    "print(f\"Dimensione: {result['audio']['size']} bytes\")\n",
    "\n",
    "len(result[\"script\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6360"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[\"script\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
